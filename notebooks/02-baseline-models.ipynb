{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Ã‰tape 3 â€“ ModÃ©lisation\n",
                "\n",
                "## 02 - ModÃ¨les Baseline pour la Classification de Radiographies COVID-19\n",
                "\n",
                "**Objectif**: Dans cette Ã©tape, nous prÃ©parons le dataset pour le machine learning classique en:\n",
                "- Encodant les labels\n",
                "- Normalisant et redimensionnant les images\n",
                "- Transformant les images en vecteurs exploitables par les modÃ¨les\n",
                "- SÃ©parant le dataset en ensembles d'entraÃ®nement et de test\n",
                "\n",
                "Nous entraÃ®nons ensuite un modÃ¨le baseline (Random Forest) pour Ã©tablir une rÃ©fÃ©rence de performance, puis nous l'optimisons."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================\n",
                "# IMPORTS\n",
                "# =============================================\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import cv2\n",
                "from collections import Counter\n",
                "\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, \n",
                "    confusion_matrix, \n",
                "    classification_report,\n",
                "    f1_score\n",
                ")\n",
                "\n",
                "# Settings\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('viridis')\n",
                "%matplotlib inline\n",
                "\n",
                "print(\"Imports OK\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-section",
            "metadata": {},
            "source": [
                "## STEP 1 â€“ BASELINE : Encodage des labels et train/test split\n",
                "\n",
                "âš ï¸ **PrÃ©requis**: Ce notebook nÃ©cessite que vous ayez exÃ©cutÃ© le notebook `01-projet.ipynb` et que les variables `X_list` et `df_images` soient disponibles.\n",
                "\n",
                "Si ce n'est pas le cas, exÃ©cutez la cellule suivante pour charger les donnÃ©es."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================\n",
                "# CHARGEMENT DES DONNÃ‰ES (si nÃ©cessaire)\n",
                "# =============================================\n",
                "# Si X_list et df_images ne sont pas dÃ©finis, charger les donnÃ©es\n",
                "\n",
                "try:\n",
                "    X_list\n",
                "    df_images\n",
                "    print(\"âœ… DonnÃ©es dÃ©jÃ  chargÃ©es\")\n",
                "except NameError:\n",
                "    print(\"â³ Chargement des donnÃ©es depuis le disque...\")\n",
                "    \n",
                "    DATA_DIR = \"../data/COVID-19_Radiography_Dataset\"\n",
                "    classes = ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
                "    \n",
                "    X_list = []\n",
                "    labels = []\n",
                "    \n",
                "    for class_name in classes:\n",
                "        class_dir = os.path.join(DATA_DIR, class_name, 'images')\n",
                "        if os.path.exists(class_dir):\n",
                "            for img_file in os.listdir(class_dir):\n",
                "                if img_file.endswith('.png'):\n",
                "                    img_path = os.path.join(class_dir, img_file)\n",
                "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
                "                    if img is not None:\n",
                "                        X_list.append(img)\n",
                "                        labels.append(class_name)\n",
                "    \n",
                "    df_images = pd.DataFrame({'label': labels})\n",
                "    print(f\"âœ… ChargÃ© {len(X_list)} images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step1-baseline-prep",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================\n",
                "# STEP 1 â€“ BASELINE : Encodage des labels et train/test split (toutes les images)\n",
                "# =============================================\n",
                "\n",
                "# ------------------------------\n",
                "# 1ï¸âƒ£ RÃ©cupÃ©ration de toutes les images et labels\n",
                "# ------------------------------\n",
                "# Objectif : utiliser tout le dataset (~21 165 images)\n",
                "X_all = np.array(X_list, dtype=np.float32)  # images prÃ©-traitÃ©es\n",
                "y_labels_all = df_images['label'].values   # labels originaux\n",
                "\n",
                "print(\"Nombre total d'images :\", X_all.shape[0])\n",
                "\n",
                "# ------------------------------\n",
                "# 2ï¸âƒ£ Encodage des labels\n",
                "# ------------------------------\n",
                "# Les modÃ¨les ML classiques ne comprennent que des valeurs numÃ©riques\n",
                "le = LabelEncoder()\n",
                "y_all = le.fit_transform(y_labels_all)\n",
                "print(\"Labels encodÃ©s :\", np.unique(y_all))\n",
                "print(\"Classes :\", le.classes_)\n",
                "print(\"Distribution des labels :\", Counter(y_labels_all))\n",
                "\n",
                "# ------------------------------\n",
                "# 3ï¸âƒ£ Flatten pour ML classique (sklearn) - Version 256x256\n",
                "# ------------------------------\n",
                "# Les modÃ¨les sklearn comme RandomForest ou SVM attendent des vecteurs 1D\n",
                "# Chaque image 2D (256x256) est transformÃ©e en vecteur de taille 65536 (256*256)\n",
                "X_flat_all = X_all.reshape(X_all.shape[0], -1)\n",
                "print(\"Shape de X_flat pour ML (256x256) :\", X_flat_all.shape)\n",
                "\n",
                "# ------------------------------\n",
                "# 4ï¸âƒ£ SÃ©paration train/test\n",
                "# ------------------------------\n",
                "# Objectif : entraÃ®ner le modÃ¨le sur une partie du dataset et tester sur le reste\n",
                "# stratify=y_all assure que la rÃ©partition des classes reste Ã©quilibrÃ©e dans train et test\n",
                "X_train_full, X_test_full, y_train, y_test = train_test_split(\n",
                "    X_flat_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
                ")\n",
                "\n",
                "print(f\"Train size: {X_train_full.shape[0]}, Test size: {X_test_full.shape[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step1-baseline-model-title",
            "metadata": {},
            "source": [
                "## STEP 1 â€“ BASELINE MODEL OPTIMISE : Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step1-baseline-model",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================\n",
                "# STEP 1 â€“ BASELINE MODEL OPTIMISE : Random Forest\n",
                "# =============================================\n",
                "\n",
                "print(\"=== Ã‰tape 1ï¸âƒ£ : Redimensionnement des images Ã  64x64 ===\")\n",
                "# ------------------------------\n",
                "# 1ï¸âƒ£ Redimensionnement des images Ã  64x64\n",
                "# ------------------------------\n",
                "# On utilise des images plus petites pour accÃ©lÃ©rer l'entraÃ®nement\n",
                "TARGET_SIZE = (64, 64)\n",
                "X_small = np.array([cv2.resize(img, TARGET_SIZE) for img in X_list], dtype=np.float32) / 255.0\n",
                "print(\"Shape de X_small :\", X_small.shape)  # (21165, 64, 64)\n",
                "\n",
                "print(\"=== Ã‰tape 2ï¸âƒ£ : Flatten des images pour ML classique ===\")\n",
                "# ------------------------------\n",
                "# 2ï¸âƒ£ Flatten pour ML classique\n",
                "# ------------------------------\n",
                "X_flat = X_small.reshape(X_small.shape[0], -1)\n",
                "print(\"Shape de X_flat pour ML :\", X_flat.shape)  # 21165, 4096\n",
                "\n",
                "print(\"=== Ã‰tape 3ï¸âƒ£ : Encodage des labels ===\")\n",
                "# ------------------------------\n",
                "# 3ï¸âƒ£ Encodage des labels\n",
                "# ------------------------------\n",
                "le = LabelEncoder()\n",
                "y = le.fit_transform(df_images['label'].values)\n",
                "print(\"Labels encodÃ©s :\", np.unique(y))\n",
                "print(\"Classes :\", le.classes_)\n",
                "\n",
                "print(\"=== Ã‰tape 4ï¸âƒ£ : SÃ©paration train/test ===\")\n",
                "# ------------------------------\n",
                "# 4ï¸âƒ£ Train/test split\n",
                "# ------------------------------\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_flat, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\n",
                "\n",
                "print(\"=== Ã‰tape 5ï¸âƒ£ : EntraÃ®nement du modÃ¨le Random Forest ===\")\n",
                "# ------------------------------\n",
                "# 5ï¸âƒ£ Random Forest Baseline\n",
                "# ------------------------------\n",
                "rf_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)  # n_jobs=-1 pour utiliser tous les coeurs\n",
                "rf_model.fit(X_train, y_train)\n",
                "\n",
                "print(\"=== Ã‰tape 6ï¸âƒ£ : Ã‰valuation du modÃ¨le ===\")\n",
                "# PrÃ©dictions\n",
                "y_pred = rf_model.predict(X_test)\n",
                "\n",
                "# Ã‰valuation\n",
                "acc = accuracy_score(y_test, y_pred)\n",
                "print(f\"Accuracy sur le jeu test : {acc:.4f}\")\n",
                "\n",
                "# Matrice de confusion\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
                "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
                "plt.title(\"Matrice de confusion â€“ Random Forest Baseline\")\n",
                "plt.xlabel(\"PrÃ©dictions\")\n",
                "plt.ylabel(\"VÃ©ritÃ©s\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Classification report\n",
                "print(\"\\nClassification Report :\\n\")\n",
                "print(classification_report(y_test, y_pred, target_names=le.classes_))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step1-f1-scores",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ------------------------------\n",
                "# Calcul des F1-scores\n",
                "# ------------------------------\n",
                "# F1-score macro : moyenne des F1 de chaque classe (Ã©quilibre toutes les classes)\n",
                "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
                "\n",
                "# F1-score micro : considÃ¨re tous les Ã©chantillons Ã©galement (pondÃ©rÃ© par support)\n",
                "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
                "\n",
                "print(f\"F1-score macro : {f1_macro:.4f}\")\n",
                "print(f\"F1-score micro : {f1_micro:.4f}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Commentaire :\")\n",
                "print(\"=\"*60)\n",
                "print(\"Le modÃ¨le Random Forest baseline montre de bonnes performances globales.\")\n",
                "print(\"Il prÃ©dit bien les classes majoritaires comme 'Normal', mais les classes\")\n",
                "print(\"moins reprÃ©sentÃ©es ou plus difficiles Ã  distinguer, comme 'COVID', ont\")\n",
                "print(\"un rappel plus faible. Une optimisation ciblÃ©e pourrait amÃ©liorer ces classes.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step2-title",
            "metadata": {},
            "source": [
                "## STEP 2 â€“ Optimisation du modÃ¨le Random Forest\n",
                "\n",
                "Nous allons maintenant optimiser les hyperparamÃ¨tres du modÃ¨le Random Forest pour amÃ©liorer ses performances."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step2-optimization",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# STEP 2 â€“ Optimisation + MÃ©triques (VERSION OPTIMISÃ‰E)\n",
                "# ============================================================\n",
                "\n",
                "print(\"\\n=== STEP 2 : OPTIMISATION DU MODELE ===\")\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 1) HyperparamÃ¨tres Ã  tester (grille rÃ©duite pour Ã©viter blocage)\n",
                "# ------------------------------------------------------------\n",
                "# Au lieu de tester 81 combinaisons comme un GridSearch (3x3x3x3),\n",
                "# on rÃ©duit volontairement pour que le PC ne plante pas.\n",
                "param_dist = {\n",
                "    'n_estimators': [100, 150],       # nombre d'arbres raisonnable\n",
                "    'max_depth': [None, 15],          # profondeur contrÃ´lÃ©e\n",
                "    'min_samples_split': [2, 5],      # valeurs simples\n",
                "    'min_samples_leaf': [1, 2]        # Ã©vite le surapprentissage\n",
                "}\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 2) Configuration de RandomizedSearchCV\n",
                "# ------------------------------------------------------------\n",
                "# RandomizedSearch teste un Ã©chantillon alÃ©atoire de combinaisons\n",
                "# â†’ beaucoup plus lÃ©ger que GridSearchCV\n",
                "optim_rf = RandomizedSearchCV(\n",
                "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
                "    param_distributions=param_dist,\n",
                "    n_iter=5,                # Nombre total de modÃ¨les testÃ©s â†’ trÃ¨s lÃ©ger\n",
                "    scoring='accuracy',\n",
                "    cv=3,\n",
                "    verbose=1,\n",
                "    random_state=42,\n",
                "    n_jobs=-1                # Utilise tous les cÅ“urs sans bloquer (car grille rÃ©duite)\n",
                ")\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 3) EntraÃ®nement\n",
                "# ------------------------------------------------------------\n",
                "print(\"\\nRecherche des meilleurs hyperparamÃ¨tres...\")\n",
                "optim_rf.fit(X_train, y_train)\n",
                "\n",
                "# ModÃ¨le optimal trouvÃ©\n",
                "best_rf = optim_rf.best_estimator_\n",
                "print(\"ğŸŸ© Meilleurs paramÃ¨tres trouvÃ©s :\", optim_rf.best_params_)\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 4) Ã‰valuation du modÃ¨le optimisÃ©\n",
                "# ------------------------------------------------------------\n",
                "print(\"\\n=== EVALUATION ===\")\n",
                "\n",
                "y_pred_opt = best_rf.predict(X_test)\n",
                "\n",
                "acc_opt = accuracy_score(y_test, y_pred_opt)\n",
                "print(f\"Accuracy : {acc_opt:.4f}\")\n",
                "\n",
                "print(\"\\nMatrice de confusion :\")\n",
                "print(confusion_matrix(y_test, y_pred_opt))\n",
                "\n",
                "print(\"\\nRapport de classification :\")\n",
                "print(classification_report(y_test, y_pred_opt, target_names=le.classes_))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step2-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================\n",
                "# STEP 2 â€“ Suite : Analyse dÃ©taillÃ©e et visualisation\n",
                "# =============================================================\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 1) Calcul des F1-scores macro et micro\n",
                "# ------------------------------------------------------------\n",
                "f1_macro_opt = f1_score(y_test, y_pred_opt, average='macro')\n",
                "f1_micro_opt = f1_score(y_test, y_pred_opt, average='micro')\n",
                "\n",
                "print(f\"\\nF1-score macro : {f1_macro_opt:.4f}\")\n",
                "print(f\"F1-score micro : {f1_micro_opt:.4f}\")\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 2) Affichage matrice de confusion graphique\n",
                "# ------------------------------------------------------------\n",
                "cm_opt = confusion_matrix(y_test, y_pred_opt)\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm_opt, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
                "plt.xlabel('PrÃ©dictions')\n",
                "plt.ylabel('VÃ©ritÃ©s')\n",
                "plt.title('Matrice de confusion â€“ Random Forest optimisÃ©')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# ------------------------------------------------------------\n",
                "# 3) Tableau synthÃ©tique par classe\n",
                "# ------------------------------------------------------------\n",
                "report = classification_report(y_test, y_pred_opt, target_names=le.classes_, output_dict=True)\n",
                "print(\"\\n=== Tableau synthÃ©tique par classe ===\")\n",
                "for cls in le.classes_:\n",
                "    precision = report[cls]['precision']\n",
                "    recall = report[cls]['recall']\n",
                "    f1 = report[cls]['f1-score']\n",
                "    support = report[cls]['support']\n",
                "    print(f\"{cls:15s} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f} | Support: {int(support)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step2-misclassified",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ------------------------------------------------------------\n",
                "# 4) Visualisation qualitative : images mal classÃ©es\n",
                "# ------------------------------------------------------------\n",
                "\n",
                "# Indices des erreurs\n",
                "errors_idx = np.where(y_pred_opt != y_test)[0]\n",
                "\n",
                "print(f\"\\nNombre d'images mal classÃ©es : {len(errors_idx)} / {len(y_test)}\")\n",
                "\n",
                "# Afficher 6 premiÃ¨res erreurs\n",
                "plt.figure(figsize=(12, 6))\n",
                "for i, idx in enumerate(errors_idx[:6]):\n",
                "    plt.subplot(2, 3, i+1)\n",
                "    plt.imshow(X_test[idx].reshape(64, 64), cmap='gray')\n",
                "    plt.title(f\"Vrai: {le.classes_[y_test[idx]]}\\nPred: {le.classes_[y_pred_opt[idx]]}\")\n",
                "    plt.axis('off')\n",
                "plt.suptitle(\"Exemples d'images mal classÃ©es\", fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison-title",
            "metadata": {},
            "source": [
                "## Comparaison Baseline vs OptimisÃ©"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "comparison",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================\n",
                "# COMPARAISON : Baseline vs OptimisÃ©\n",
                "# =============================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"COMPARAISON DES PERFORMANCES\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "comparison_data = {\n",
                "    'ModÃ¨le': ['Random Forest Baseline', 'Random Forest OptimisÃ©'],\n",
                "    'Accuracy': [f\"{acc:.4f}\", f\"{acc_opt:.4f}\"],\n",
                "    'F1-macro': [f\"{f1_macro:.4f}\", f\"{f1_macro_opt:.4f}\"],\n",
                "    'F1-micro': [f\"{f1_micro:.4f}\", f\"{f1_micro_opt:.4f}\"]\n",
                "}\n",
                "\n",
                "df_comparison = pd.DataFrame(comparison_data)\n",
                "print(df_comparison.to_string(index=False))\n",
                "\n",
                "# Visualisation de la comparaison\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "metrics = ['Accuracy', 'F1-macro', 'F1-micro']\n",
                "baseline_vals = [acc, f1_macro, f1_micro]\n",
                "optimized_vals = [acc_opt, f1_macro_opt, f1_micro_opt]\n",
                "\n",
                "for i, metric in enumerate(metrics):\n",
                "    ax = axes[i]\n",
                "    bars = ax.bar(['Baseline', 'OptimisÃ©'], [baseline_vals[i], optimized_vals[i]], \n",
                "                  color=['steelblue', 'darkorange'])\n",
                "    ax.set_title(metric, fontsize=12, fontweight='bold')\n",
                "    ax.set_ylim(0.7, 0.9)\n",
                "    ax.set_ylabel('Score')\n",
                "    \n",
                "    # Ajouter les valeurs sur les barres\n",
                "    for bar, val in zip(bars, [baseline_vals[i], optimized_vals[i]]):\n",
                "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
                "                f'{val:.4f}', ha='center', va='bottom', fontsize=10)\n",
                "\n",
                "plt.suptitle('Comparaison des performances : Baseline vs OptimisÃ©', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary-title",
            "metadata": {},
            "source": [
                "## RÃ©sumÃ© et Conclusions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "summary",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================\n",
                "# RÃ©sumÃ© Step 1 & Step 2 â€“ ModÃ©lisation Random Forest\n",
                "# =============================================================\n",
                "\n",
                "print(\"\"\"\n",
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "ğŸ“Š RÃ‰SUMÃ‰ DE LA MODÃ‰LISATION\n",
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "1ï¸âƒ£ OBJECTIF :\n",
                "   - Optimiser les hyperparamÃ¨tres de la Random Forest pour amÃ©liorer\n",
                "     la prÃ©cision et le F1-score sur notre dataset (~21 165 images).\n",
                "   - Analyser les performances par classe afin de comprendre les forces\n",
                "     et limites du modÃ¨le.\n",
                "\n",
                "2ï¸âƒ£ MÃ‰THODE :\n",
                "   - Redimensionnement des images Ã  64x64 et flatten pour ML classique.\n",
                "   - RandomizedSearchCV pour tester 5 combinaisons d'hyperparamÃ¨tres\n",
                "     de la Random Forest, afin de ne pas bloquer le PC.\n",
                "   - SÃ©paration train/test (80/20) et Ã©valuation complÃ¨te.\n",
                "\n",
                "3ï¸âƒ£ MEILLEURS HYPERPARAMÃˆTRES TROUVÃ‰S :\n",
                "   â†’ ForÃªt lÃ©gÃ¨rement profonde avec beaucoup d'arbres pour stabilitÃ©.\n",
                "\n",
                "4ï¸âƒ£ PERFORMANCES GLOBALES :\n",
                "   - Accuracy : ~82%\n",
                "   - F1-score macro : ~0.81-0.82\n",
                "   - F1-score micro : ~0.82-0.83\n",
                "\n",
                "5ï¸âƒ£ ANALYSE PAR CLASSE :\n",
                "   | Classe           | Precision | Recall | F1-score |\n",
                "   |-----------------|-----------|--------|----------|\n",
                "   | COVID            | ~0.90     | ~0.69  | ~0.78    |\n",
                "   | Lung_Opacity     | ~0.80     | ~0.74  | ~0.77    |\n",
                "   | Normal           | ~0.80     | ~0.92  | ~0.86    |\n",
                "   | Viral Pneumonia  | ~0.94     | ~0.79  | ~0.86    |\n",
                "   \n",
                "   - Les classes majoritaires (Normal, Lung_Opacity) sont bien prÃ©dites.\n",
                "   - Les classes difficiles (COVID) ont un rappel plus faible, certaines images sont mal classÃ©es.\n",
                "\n",
                "6ï¸âƒ£ VISUALISATION QUALITATIVE :\n",
                "   - Les images mal classÃ©es permettent d'identifier les confusions principales\n",
                "     entre COVID et Normal, illustrant les limites du modÃ¨le ML classique.\n",
                "\n",
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "ğŸ“‹ CONCLUSION PÃ‰DAGOGIQUE :\n",
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "   âœ… La prÃ©cision globale est correcte (~82%).\n",
                "   âš ï¸ Certaines classes comme 'COVID' et 'Normal' sont plus difficiles Ã  distinguer.\n",
                "   âš ï¸ Le modÃ¨le Random Forest a atteint sa limite sur ces images.\n",
                "   \n",
                "   ğŸ” POURQUOI CES LIMITES ?\n",
                "   Les features (pixels flattened) ne capturent pas :\n",
                "   âŒ La LOCALISATION des opacitÃ©s pulmonaires\n",
                "   âŒ Les PATTERNS SPATIAUX caractÃ©ristiques du COVID (ground-glass opacity)\n",
                "   âŒ Les STRUCTURES ANATOMIQUES (poumons, mÃ©diastin, cÃ´tes)\n",
                "\n",
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "ğŸ“‹ RECOMMANDATIONS POUR AMÃ‰LIORER :\n",
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\n",
                "   1. FEATURES SPATIALES :\n",
                "      â€¢ HOG (Histogram of Oriented Gradients) - capture les contours locaux\n",
                "      â€¢ LBP (Local Binary Patterns) - capture les textures locales\n",
                "   \n",
                "   2. DEEP LEARNING :\n",
                "      â€¢ CNN prÃ©-entraÃ®nÃ© (ResNet, VGG, EfficientNet)\n",
                "      â€¢ Transfer learning depuis ImageNet\n",
                "   \n",
                "   3. SEGMENTATION :\n",
                "      â€¢ Utiliser les masques pulmonaires disponibles dans data/*/masks/\n",
                "      â€¢ Focaliser l'analyse sur les rÃ©gions pulmonaires uniquement\n",
                "\n",
                "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "\"\"\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
